{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "static-interest",
   "metadata": {},
   "source": [
    "# Variational classifier with PennyLane\n",
    "\n",
    "This is a [Xanadu tutorial](https://pennylane.ai/qml/demos/tutorial_variational_classifier.html) for PennyLane's library wich has the purpose to implement a variational quantum classifier. Here we will train a quantum circuit with labelled data and then use it to classify new data.\n",
    "\n",
    "It can be shown that de variational quantum classifier can reproduce the parity function\n",
    "\n",
    "$$f : x\\in \\{0,1\\} ^{\\otimes n} \\rightarrow y = \\left\\{\\begin{array}{lll}\n",
    "1& \\text{if uneven number of ones in } x\\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "I think this parity function is not unique, so then we could construct the function depending on what we would like to classify.\n",
    "\n",
    "Here it is shown how to encode real vectors as amplitud vectors, this is called amplitude encoding, and with this we can train the model to recognize the first two classes of flowers in the Iris dataset.\n",
    "\n",
    "--------\n",
    "\n",
    "We begin by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "skilled-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documented-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now initialice a quantum circuit with 4 qubits\n",
    "dev = qml.device(\"default.qubit\", wires = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-demonstration",
   "metadata": {},
   "source": [
    "Now we have to define our layer or block. This is an elementary circuit architecture that is repeated to build the variational circuit. This layers can be modified according on what we need. The Layer proposed in this guide consist on arbitrary rotations on each qubit as well as CNOT gates that entangle each qubit with its neightbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deluxe-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "    #qml.Rot(\\phi,\\theta,\\omega,wire)\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
    "    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n",
    "    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    qml.CNOT(wires=[3, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-medicare",
   "metadata": {},
   "source": [
    "Now we have to find the way to encode data inputs $x$ into the circuit, then, we have to define a quantum state of qubits which represents the $x$ imput. In this case the inputs are bitstrings, so directly they define\n",
    "$$ x = 0101 \\rightarrow |\\psi\\rangle = |0101\\rangle $$\n",
    "There is a function called BasisState. Which prepares n wires in a given list of zeros and ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unlike-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statePreparation(x):\n",
    "    #x is a np.array, for example [0,0,1,1]\n",
    "    #the function prepares the wires in the state |0011>\n",
    "    \n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-baltimore",
   "metadata": {},
   "source": [
    "Now a Qnode is defined as a routine for the state preparation, followed by a repetition of the layer structure. So, given a determined amount of weights (which are the analogous of the $\\theta$ parameters) the layer is repeated using each of this weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "floppy-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights,x):\n",
    "    \n",
    "    \n",
    "    statePreparation(x)\n",
    "    \n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-macedonia",
   "metadata": {},
   "source": [
    "In this case we are giving to the Qnode the data as a keyword argument x. This Keyword arguments are fixed when calculating a gradient, this means that the traning is done over the weights!!\n",
    "\n",
    "Also we can add a \"classical\" bias, this is the amount of error from the algorithm to predict some value, it is important to take into account that low bias does not necessarily means that the algorithm is good, since when there is an overfitted algorithm the bias tends to 0. So now we built up the classifier, var will have the variables that we need, this is the weights and the bias, then we put all together in the ciruit to generate the classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "grand-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(var, x):\n",
    "    \n",
    "    weights = var[0]\n",
    "    bias = var[1]\n",
    "    return circuit(weights, x) + bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-oakland",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "Now we calculate the cost function or the Residual Sum Squares (RSS), this is the parameter that shows the error between the predicted and the real value of a datapoint first we calculate the swuare loss between real labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "expensive-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-authority",
   "metadata": {},
   "source": [
    "Now we can define de accuracy function, this function will give us the percentage of correctly predicted labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bizarre-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-prophet",
   "metadata": {},
   "source": [
    "Now we calculate the cost function as the square loss between the real labels and the predictions from the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "optical-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(var, X, Y):\n",
    "    predictions = [variational_classifier(var, x) for x in X]\n",
    "    return square_loss(Y, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-square",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "Here we are going to load the data from the file `variational_classifier/data/parity.txt`. This data has five columns where, the first four contain the information of the stat, this means the $x$ that we define earlier, and the fifth column is $1$ if $x$ has an uneven numer of ones or $0$ otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "governing-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [0. 0. 0. 0.], Y = -1\n",
      "X = [0. 0. 0. 1.], Y =  1\n",
      "X = [0. 0. 1. 0.], Y =  1\n",
      "X = [0. 0. 1. 1.], Y = -1\n",
      "X = [0. 1. 0. 0.], Y =  1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"variational_classifier/data/parity.txt\")\n",
    "X = np.array(data[:, :-1], requires_grad=False)\n",
    "Y = np.array(data[:, -1], requires_grad=False)\n",
    "Y = Y * 2 - np.ones(len(Y))  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"X = {}, Y = {: d}\".format(X[i], int(Y[i])))\n",
    "\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-transcription",
   "metadata": {},
   "source": [
    "Now we initialize random variables in order to test the algorithm. Here we generate random numbers where the first one is the bias and the other are going to be the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "competitive-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.01764052,  0.00400157,  0.00978738],\n",
      "         [ 0.02240893,  0.01867558, -0.00977278],\n",
      "         [ 0.00950088, -0.00151357, -0.00103219],\n",
      "         [ 0.00410599,  0.00144044,  0.01454274]],\n",
      "\n",
      "        [[ 0.00761038,  0.00121675,  0.00443863],\n",
      "         [ 0.00333674,  0.01494079, -0.00205158],\n",
      "         [ 0.00313068, -0.00854096, -0.0255299 ],\n",
      "         [ 0.00653619,  0.00864436, -0.00742165]]], requires_grad=True), 0.0)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "var_init = (0.01 * np.random.randn(num_layers, num_qubits, 3), 0.0)\n",
    "\n",
    "print(var_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-synthetic",
   "metadata": {},
   "source": [
    "Next step is to create the optimizer, this is provide by PannyLane as  [`NesterovMomentumOptimizer`](https://pennylane.readthedocs.io/en/stable/code/api/pennylane.NesterovMomentumOptimizer.html#pennylane.NesterovMomentumOptimizer). This is a class that optimize the gradien-descent algorithm using what is called the Nesterov momentum. Then we define the batch_size and train the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "common-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-sewing",
   "metadata": {},
   "source": [
    "Here we are going to look at the accuracy of the algorithm, taking the correctly classified data. For this we first compute what should be the correct output and then we take the prediction as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aging-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.9978697 | Accuracy: 0.5000000 \n",
      "Iter:     2 | Cost: 2.6040860 | Accuracy: 0.5000000 \n",
      "Iter:     3 | Cost: 1.9514070 | Accuracy: 0.5000000 \n",
      "Iter:     4 | Cost: 1.2102827 | Accuracy: 0.5000000 \n",
      "Iter:     5 | Cost: 0.6132715 | Accuracy: 0.8750000 \n",
      "Iter:     6 | Cost: 0.5385945 | Accuracy: 0.7500000 \n",
      "Iter:     7 | Cost: 0.1777492 | Accuracy: 1.0000000 \n",
      "Iter:     8 | Cost: 0.0799840 | Accuracy: 1.0000000 \n",
      "Iter:     9 | Cost: 0.0952246 | Accuracy: 1.0000000 \n",
      "Iter:    10 | Cost: 0.0450807 | Accuracy: 1.0000000 \n",
      "Iter:    11 | Cost: 0.0380732 | Accuracy: 1.0000000 \n",
      "Iter:    12 | Cost: 0.0224965 | Accuracy: 1.0000000 \n",
      "Iter:    13 | Cost: 0.0261320 | Accuracy: 1.0000000 \n",
      "Iter:    14 | Cost: 0.0124017 | Accuracy: 1.0000000 \n",
      "Iter:    15 | Cost: 0.0289288 | Accuracy: 1.0000000 \n",
      "Iter:    16 | Cost: 0.0104481 | Accuracy: 1.0000000 \n",
      "Iter:    17 | Cost: 0.0148988 | Accuracy: 1.0000000 \n",
      "Iter:    18 | Cost: 0.0103778 | Accuracy: 1.0000000 \n",
      "Iter:    19 | Cost: 0.0066370 | Accuracy: 1.0000000 \n",
      "Iter:    20 | Cost: 0.0040687 | Accuracy: 1.0000000 \n",
      "Iter:    21 | Cost: 0.0024308 | Accuracy: 1.0000000 \n",
      "Iter:    22 | Cost: 0.0020359 | Accuracy: 1.0000000 \n",
      "Iter:    23 | Cost: 0.0010956 | Accuracy: 1.0000000 \n",
      "Iter:    24 | Cost: 0.0009090 | Accuracy: 1.0000000 \n",
      "Iter:    25 | Cost: 0.0017125 | Accuracy: 1.0000000 \n",
      "Iter:    26 | Cost: 0.0008389 | Accuracy: 1.0000000 \n",
      "Iter:    27 | Cost: 0.0011601 | Accuracy: 1.0000000 \n",
      "Iter:    28 | Cost: 0.0012001 | Accuracy: 1.0000000 \n",
      "Iter:    29 | Cost: 0.0011997 | Accuracy: 1.0000000 \n",
      "Iter:    30 | Cost: 0.0007459 | Accuracy: 1.0000000 \n",
      "Iter:    31 | Cost: 0.0007239 | Accuracy: 1.0000000 \n",
      "Iter:    32 | Cost: 0.0005430 | Accuracy: 1.0000000 \n",
      "Iter:    33 | Cost: 0.0003817 | Accuracy: 1.0000000 \n",
      "Iter:    34 | Cost: 0.0002207 | Accuracy: 1.0000000 \n",
      "Iter:    35 | Cost: 0.0001081 | Accuracy: 1.0000000 \n"
     ]
    }
   ],
   "source": [
    "var = var_init\n",
    "\n",
    "for it in range(35):\n",
    "        \n",
    "        batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "        # Take 5 random arrays of x, i.e. the form [0,1,0,0]\n",
    "        X_batch = X[batch_index]\n",
    "        \n",
    "        # Has the answers of the previous x\n",
    "        Y_batch = Y[batch_index]\n",
    "        \n",
    "        # Computes the cost function in order to optimize it\n",
    "        var = opt.step(lambda v: cost(v, X_batch, Y_batch), var)\n",
    "        \n",
    "        # Compute the accuracy\n",
    "        predictions = [np.sign(variational_classifier(var, x)) for x in X]\n",
    "        acc = accuracy(Y, predictions)\n",
    "\n",
    "        \n",
    "        print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(var, X, Y), acc\n",
    "        )\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-instrument",
   "metadata": {},
   "source": [
    "As you can see. The cost function which is related to the diference between the predictions and the true labels decrease along the iterations, showing that the algorithm is effectively making better predictions each time. Also we can see the accuracy increasing, showing us, again, that the algorithm is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-parliament",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
